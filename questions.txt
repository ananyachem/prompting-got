# What is the Wikipedia URL to the page where your paragraph came from?

https://en.wikipedia.org/wiki/Planetary_coordinate_system



# What was your experience in obtaining correct answers? Reflect on the EM and F1 metrics in your answer and any alternative definitions of correctness in comparison to EM and F1 that you believe are relevant.

An EM score of 100% was difficult to achieve, although not impossible. Even though the answers given by the model were essentially correct, since it was not using the exact sentence as in the ground truth file, a perfect score was difficult to achieve. The F1 score was more lenient in evaluation, since the precision of the model was higher than the EM score sometimes. This resulted in the F1 score being higher than the EM score in some cases, when the EM score was less than 100%. A BLEU score can also be used which could make the evaluation metric more flexible. On testing with BLEU, I obtained a score of 94.2203, in one case where the EM and F1 score was 90% for GPT-4o-mini. 



# Describe how you experimented with different prompts to obtain your results? Consider the first prompt you used, and your final prompt, and the intermediate changes that you made and how those affected your results.

- When I first ran the model, it provided answers which essentially was the right one, but it was using its own words to create the answer. It also did this for the unanswerable questions, either providing a legitimate answer or saying something other than "no answer". This resulted in the EM score being zero.

- To correct this, I first told the model to reply with "no answer" if there was no answer present in the given context. This improved the EM score to 50%. It only got the unanswerable questions right.

- Next, I told the model to answer questions using only the exact sentences from the context. This improved the EM score to 70-80%. The mismatched answers were the ones that had only a part of the sentence as the answer, like a phrase for example. The model was using the full sentence from the context which contained the right answer, instead of answering straight to the point.

- Lastly, I prompted the model to answer with just a phrase where necessary, and to not include any extra information that isn't asked for. The only problem here was with casing and punctuation. It would not flag the answer as correct, if there was a mis-matching capital letter or a period at the end of a sentence. To remedy this, I removed all punctuation and casing before checking for the final accuracy. This gave me an EM score of 90%. The F1 score was also 90%.  

- This score wasn't consistent, since sometimes it even answered an unanswerable question. To fix this, I arranged the order of my prompts, telling it to reply with "no answer" as the last step, so that this instruction stays fresh. This gave me an EM and F1 score of 100%. I also changed the opening prompt of "You are great at answering questions from a given context" to "You are a helpful assistant". What I assume happened is that, by saying that it is great at answering questions, it sometimes wanted to answer even the unanswerable questions, living up to that expectation I set. Changing this opening prompt, helped me achieve a perfect score.  

With this I concluded in fine-tuning my model to answer exactly as I wanted.

The code was tested using GPT-4o-mini and I was able to achieve a perfect score. Running it on GPT-4o also gave me a near perfect score, but GPT-3.5-turbo did not perform as well as the other models. It gave an answer to the unanswerable questions too, despite prompting it to answer it with "no answer" in such cases, which brought down the EM score. 



# What were your average EM and F1 scores for each of the three models

gpt-4o: EM score- 90, F1 score- 90
gpt-4o-mini: EM score- 100, F1 score- 100
gpt-3.5-turbo: EM score- 60, F1 score- 67.5
